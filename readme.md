# AirV2Xâ€‘Perception

**Official implementation of**  
**â€œAirV2X: Unified Airâ€“Ground/Vehicleâ€‘toâ€‘Everything Collaboration for Perceptionâ€**


<!-- [![Paper](https://img.shields.io/badge/arXiv-Paper-red.svg)](https://arxiv.org/abs/2506.19283)
[![Project Page](https://img.shields.io/badge/Project-Page-1f72ff.svg)](https://xiangbogaobarry.github.io/AirV2X/)
[![Code](https://img.shields.io/badge/GitHub-Code-black.svg)](https://github.com/taco-group/AirV2X-Perception)
[![Dataset](https://img.shields.io/badge/HuggingFace-Dataset-yellow.svg)](https://huggingface.co/datasets/xiangbog/AirV2X-Perception) 
-->

---

## ğŸŒ Dataset

Download **AirV2Xâ€‘Perception** from Hugging Face and extract it to any location:

```bash
mkdir dataset
cd dataset # Use another directory to avoid naming conflict
conda install -c conda-forge git-lfs
git lfs install --skip-smudge
git clone https://huggingface.co/datasets/xiangbog/AirV2X-Perception
cd AirV2X-Perception
git lfs pull
# git lfs pull --include "path/to/folder"   # If you would like to download only partial of the dataset
```

We also provide a *mini batch* for quick testing and debugging.

---

## ğŸ”§ Installation

Detailed instructions and environment specifications are in [`doc/INSTALL.md`](doc/INSTALL.md).

---

## ğŸš€ Model Training

### Singleâ€‘GPU

```bash
python opencood/tools/train.py \
    -y /path/to/config_file.yaml
```

Example: train **Where2Comm** (LiDARâ€‘only)

```bash
python opencood/tools/train.py \
    -y opencood/hypes_yaml/airv2x/lidar/det/airv2x_intermediate_where2com.yaml
```

> **Tip**  
> Some models such as **V2Xâ€‘ViT** and **CoBEVT** consume a large amount of VRAM.  
> Enable mixedâ€‘precision with `--amp` if you encounter OOM, but watch out for *NaN/Inf* instability.

```bash
python opencood/tools/train.py \ 
    -y opencood/hypes_yaml/airv2x/lidar/det/airv2x_intermediate_v2xvit.yaml       
    --amp
```

### Multiâ€‘GPU (DDP)

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun \
    --standalone --nproc_per_node=4 \     
    opencood/tools/train.py \
        -y /path/to/config_file.yaml
```

Example: LiDARâ€‘only **Where2Comm** with 8 GPUs

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun \       
    --standalone\
    --nproc_per_node=8 \
    opencood/tools/train.py \
        -y opencood/hypes_yaml/airv2x/lidar/det/airv2x_intermediate_where2com.yaml
```

### Multiâ€‘Stage Models (HEAL, STAMP)

These models were trained on **2 nodes Ã— 1 GPU (batch sizeÂ 1)**.  
If you change the number of GPUs or batch size, adjust the learning rate accordingly.

---

## ğŸ“ Evaluation

```bash
python opencood/tools/inference_multi_scenario.py \ 
    --model_dir opencood/logs/airv2x_intermediate_where2comm/default__2025_07_10_09_17_28 \
    --eval_best_epoch \
    --save_vis
```

---

## ğŸ“¦ Preâ€‘Trained Checkpoints

| Modality | Model | AP@0.3 | AP@0.5 | AP@0.7 | Config & Checkpoint |
|---|---|---|---|---|---|
| **LiDAR** | When2Com | 0.1824 | 0.0787 | 0.0025 | *Coming soon* |
|  | CoBEVT | **0.4922** | **0.4585** | **0.2582** | [HF](https://huggingface.co/xiangbog/AirV2X-Perception-Checkpoints/tree/main/airv2x_intermediate_cobevt/release) |
|  | Where2Comm | 0.4366 | 0.4015 | 0.1538 | [HF](https://huggingface.co/xiangbog/AirV2X-Perception-Checkpoints/tree/main/airv2x_intermediate_where2comm/release) |
|  | V2Xâ€‘ViT | 0.4401 | 0.3821 | 0.1638 | [HF](https://huggingface.co/xiangbog/AirV2X-Perception-Checkpoints/tree/main/airv2x_intermediate_v2xvit/release) |

> **Note**  
> The above checkpoints were trained for **50Â epochs** with `batch_size=2`, so their numbers may differ slightly from the paper.

Additional Cameraâ€‘only and Multiâ€‘modal checkpoints are on the way.

---

## ğŸ” Visualization

```bash
tensorboard --logdir opencood/logs --port 10000 --bind_all
```

---

## ğŸ“„ Citation

```bibtex
@article{gao2025airv2x,
  title   = {AirV2X: Unified Air--Ground/Vehicle-to-Everything Collaboration for Perception},
  author  = {Gao, Xiangbo and Tu, Zhengzhong and others},
  journal = {arXiv preprint arXiv:2506.19283},
  year    = {2025}
}
```

---

We will continuously update this repository with code, checkpoints, and documentation.  
Feel free to open issues or pull requests â€” contributions are welcome! ğŸš€
